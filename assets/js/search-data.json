{
  
    
        "post0": {
            "title": "Reinforcement learning introduction - Solution",
            "content": ". Reinforcement learning introduction: Frozen Lake example . The Frozen Lake example is a classic reinforcement learning problem, where the goal is to teach an agent to navigate a frozen lake and reach the goal without falling through the ice. . . In this example, we use the Q-learning algorithm to train an agent to navigate the FrozenLake environment. The Q-learning algorithm works by learning a Q-table, which is a table that maps each state-action pair to a value that represents the expected future reward of taking that action in that state. The Q-table is initialized to zeros, and is updated over time based on the rewards that the agent receives for taking actions in different states. . The training process involves repeatedly running episodes, where each episode consists of the agent taking actions in the environment until it reaches the goal or falls in a hole. During each time step of an episode, the agent selects an action based on the current state and the Q-table, and then takes that action and observes the resulting reward and next state. The Q-table is then updated based on the observed reward and the expected future reward of the next state, according to the Q-learning update rule. . Once the agent has been trained, we test its performance on a set of test episodes. During each test episode, the agent takes actions in the environment using the Q-table that was learned during training, and we observe whether it is able to reach the goal or not. . Overall, the Frozen Lake example is a simple but illustrative example of how reinforcement learning can be used to train an agent to navigate an environment and accomplish a task. . . . . . . . . Description: . The Frozen Lake environment is a grid of frozen ice, holes, and a goal, here we will have (4*4) grid size, that‚Äôs mean we will have (16 STATES) each cell repersent a state. . The agent starts at the top left corner cell in the grid, and can take four actions at each time step, We will have (4 ACTIONS): . move up | move down | move left | move right | . The goal is to reach the goal cell in the bottom right corner of the grid without falling through any holes. Therefore, our REWARDS are: . If agent current state is a hole state, the reward = -1 | If agent current state is the goal state, the reward = +1 | Else, the reward = 0 | . . WE‚ÄôRE GOING TO DO TWO VERSIONS of FROZEN LAKE EXAMPLE: . . Version [1]: . In this version we use ‚ÄòGym‚Äô to simplfiy the code . Import packages and setup your environment . This code imports the FrozenLake environment from the OpenAI Gym library and creates an instance of the environment. . import gym import numpy as np import matplotlib.pyplot as plt # Create the FrozenLake environment env = gym.make(&#39;FrozenLake-v1&#39;) . This code initializes the Q-table to zeros. The Q-table is a matrix where the rows represent the possible states of the environment and the columns represent the possible actions that the agent can take. . # Initialize the Q-table to zeros Q = np.zeros([env.observation_space.n, env.action_space.n]) . Hyperparameters üìà . # Set hyperparameters lr = 0.8 # learning rate gamma = 0.95 # discount factor num_episodes = 2000 # number of training episodes . TRAIN YOUR AGENT ü§ñ ‚ùÑ . This code trains the agent using Q-learning. During training, the agent interacts with the environment by selecting actions based on the Q-table and updating the Q-table based on the observed reward. The hyperparameters lr and gamma control how much the agent values immediate rewards versus future rewards. The num_episodes parameter controls how many times the agent interacts with the environment. . # Keep track of the total reward for each episode rewards = np.zeros(num_episodes) # Train the agent using Q-learning for i in range(num_episodes): # Reset the environment for each episode s = env.reset() done = False while not done: # Choose an action based on the Q-table, with some random noise a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n)*(1./(i+1))) # Take the chosen action and observe the next state and reward s_new, r, done, _ = env.step(a) # Update the Q-table based on the observed reward Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s_new,:]) - Q[s,a]) # Add the reward to the total reward for the episode rewards[i] += r # Set the current state to the next state s = s_new . TEST YOUR AGENT üß™ . This code tests the agent on 100 episodes after training. During testing, the agent chooses actions based on the Q-table and tries to reach the goal state. The code keeps track of the number of successful episodes (where the agent reaches the goal state) and prints the success rate at the end. . # Test the agent on 100 episodes num_successes = 0 for i in range(100): s = env.reset() done = False while not done: # Choose an action based on the Q-table a = np.argmax(Q[s,:]) s, r, done, _ = env.step(a) if r == 1: num_successes += 1 # Print the success rate print(&quot;Success rate:&quot;, num_successes/100) . Success rate: 0.54 . NOW LET‚Äôs PLOT THE LEARNING üñå . # Calculate the rolling average of rewards rolling_avg_rewards = np.zeros(num_episodes) window_size = 100 for i in range(num_episodes): rolling_avg_rewards[i] = np.mean(rewards[max(0,i-window_size+1):(i+1)]) # Plot the total rewards and rolling average rewards fig, ax = plt.subplots(2, 1, figsize=(8,8)) ax[0].plot(rewards) ax[0].set_xlabel(&#39;Episode&#39;) ax[0].set_ylabel(&#39;Total reward&#39;) ax[1].plot(rolling_avg_rewards) ax[1].set_xlabel(&#39;Episode&#39;) ax[1].set_ylabel(f&#39;Rolling average reward (window size {window_size})&#39;) . Text(0, 0.5, ‚ÄòRolling average reward (window size 100)‚Äô) . . . . . . . Version [2] . . LET GET MORE TO THE DETAILS: . . Import packages . import numpy as np import random . First, we define the FrozenLake environment as a class, with methods for resetting the environment, taking actions, rendering the current state, and showing the current Q-table and policy. . class FrozenLake: def __init__(self, size=4): self.size = size self.grid = np.zeros((size, size), dtype=int) self.start_state = (0, 0) self.goal_state = (size-1, size-1) self.hole_states = [(1, 1), (2, 3), (3, 0)] for i, j in self.hole_states: self.grid[i][j] = 1 def reset(self): self.current_state = self.start_state return self.current_state def step(self, action): i, j = self.current_state if action == 0: # move up i = max(i-1, 0) elif action == 1: # move down i = min(i+1, self.size-1) elif action == 2: # move left j = max(j-1, 0) elif action == 3: # move right j = min(j+1, self.size-1) self.current_state = (i, j) if self.current_state == self.goal_state: reward = 1 done = True elif self.current_state in self.hole_states: reward = -1 done = True else: reward = 0 done = False return self.current_state, reward, done # console text printing the grid and show the agent&#39;s moves def render(self): print(&#39; n&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) elif (i, j) == self.goal_state: print(&#39;G&#39;, end=&#39; &#39;) else: print(&#39;.&#39;, end=&#39; &#39;) elif self.grid[i][j] == 1: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) else: print(&#39;X&#39;, end=&#39; &#39;) print() print() #print the Q-table of all values def show_q_table(self, q_table): print(&#39;--&#39;) print(&#39;Q-Table:&#39;) print(&#39;--&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: print( &#39;%.2f&#39; % q_table[i][j][0], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][1], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][2], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][3]) else: print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;) print() # In one text line show the policy (the sequence of actions that agent take ) def show_policy(self, q_table): print(&#39; n Policy:&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: action = np.argmax(q_table[i][j]) if action == 0: print(&#39;UP&#39;, end=&#39; &#39;) elif action == 1: print(&#39;DOWN&#39;, end=&#39; &#39;) elif action == 2: print(&#39;LEFT&#39;, end=&#39; &#39;) elif action == 3: print(&#39;RIGHT&#39;, end=&#39; &#39;) else: print(&#39;STAY&#39;, end=&#39; &#39;) . Next, we create an instance of the environment and initialize the Q-table with zeros. . # Create the environment env = FrozenLake() # Initialize Q-table with zeros q_table = np.zeros((env.size, env.size, 4)) . Hyperparameters üìà . We then set some hyperparameters for the Q-learning algorithm, such as the number of episodes to run, the maximum number of steps per episode, the learning rate, the discount factor, the starting exploration rate (epsilon), the minimum exploration rate, and the rate at which epsilon decays over time. . # Set hyperparameters num_episodes = 10000 max_steps_per_episode = 100 learning_rate = 0.1 discount_factor = 0.99 epsilon = 1.0 min_epsilon = 0.01 epsilon_decay_rate = 0.001 . We define an epsilon-greedy policy for selecting actions, which chooses a random action with probability epsilon or the greedy action (i.e., the action with the highest Q-value) with probability 1 - epsilon. . . # Define epsilon-greedy policy def epsilon_greedy_policy(state): if random.uniform(0, 1) &lt; epsilon: return random.randint(0, 3) else: return np.argmax(q_table[state[0]][state[1]]) . We train the agent by running a loop over the specified number of episodes. In each episode, we start by resetting the environment and selecting actions according to the epsilon-greedy policy. We then update the Q-values for the current state-action pair using the Q-learning update rule. Finally, we update the current state and repeat until the episode ends (either because the agent reaches the goal or exceeds the maximum number of steps). . We decay the exploration rate (epsilon) after each episode to gradually shift the agent from exploration to exploitation over time. . Periodically, we render the current state of the environment and display the current Q-table and policy for visualization. . # Train agent for episode in range(num_episodes): state = env.reset() done = False t = 0 while not done and t &lt; max_steps_per_episode: action = epsilon_greedy_policy(state) next_state, reward, done = env.step(action) q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action]) state = next_state t += 1 epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay_rate)) # Show progress if episode % 1000 == 0: env.render() env.show_q_table(q_table) env.show_policy(q_table) . . . . . . S . . . . . X X . . G -- Q-Table: -- 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.10 NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Policy: UP UP UP UP UP STAY UP UP UP UP UP STAY STAY UP UP UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.84 0.85 0.96 0.82 0.94 0.58 0.85 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.65 -0.95 0.97 0.80 0.47 -1.00 0.32 0.81 -0.95 0.72 0.45 0.98 0.97 0.99 0.96 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.58 0.71 -0.81 0.98 0.98 0.99 0.94 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.88 0.91 0.96 0.90 0.94 0.69 0.88 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.91 -0.95 0.79 0.66 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.65 0.84 -0.88 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.90 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . . . . . X . . . . . X X . . S -- Q-Table: -- 0.94 0.93 0.94 0.95 0.95 -1.00 0.94 0.96 0.96 0.97 0.95 0.95 0.89 0.91 0.96 0.91 0.94 0.73 0.89 -1.00 NULL NULL NULL NULL 0.96 0.98 -1.00 0.96 0.81 -0.96 0.97 0.90 0.56 -1.00 0.37 0.92 -0.96 0.81 0.69 0.98 0.97 0.99 0.97 -1.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.68 0.84 -0.89 0.99 0.98 0.99 0.98 1.00 0.00 0.00 0.00 0.00 Policy: RIGHT RIGHT DOWN LEFT UP STAY DOWN LEFT RIGHT RIGHT DOWN STAY STAY RIGHT RIGHT UP . Once training is complete, we test the agent by running a loop until the agent reaches the goal or exceeds the maximum number of steps. In each step, we select the greedy action (i.e., the action with the highest Q-value) and update the current state. We render the environment at each step for visualization. . # Test agent state = env.reset() done = False while not done: action = np.argmax(q_table[state[0]][state[1]]) next_state, reward, done = env.step(action) env.render() state = next_state . . S . . . X . . . . . X X . . G . . S . . X . . . . . X X . . G . . . . . X S . . . . X X . . G . . . . . X . . . . S X X . . G . . . . . X . . . . . X X . S G . . . . . X . . . . . X X . . S . .",
            "url": "https://sciml-leeds.github.io/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Solution.html",
            "relUrl": "/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Solution.html",
            "date": " ‚Ä¢ May 5, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Reinforcement learning introduction - Challenge",
            "content": ". Reinforcement learning introduction: Frozen Lake example . The Frozen Lake example is a classic reinforcement learning problem, where the goal is to teach an agent to navigate a frozen lake and reach the goal without falling through the ice. . . Description: . The Frozen Lake environment is a grid of frozen ice, holes, and a goal, here we will have (4*4) grid size, that‚Äôs mean we will have (16 STATES) each cell represent a state. . The agent starts at the top left corner cell in the grid, and can take four actions at each time step, We will have (4 ACTIONS): . move up | move down | move left | move right | . The goal is to reach the goal cell in the bottom right corner of the grid without falling through any holes. Therefore, our REWARDS are: . If agent current state is a hole state, the reward = -1 | If agent current state is the goal state, the reward = +1 | Else, the reward = 0 | . . . . Import packages and setup your environment . import numpy as np import random . First, we define the FrozenLake environment as a class, with methods for resetting the environment, taking actions, rendering the current state, and showing the current Q-table and policy. . class FrozenLake: def __init__(self, size=4): self.size = size self.grid = np.zeros((4, 4), dtype=int) # spcifiy grid size please self.start_state = (0, 0) self.goal_state = (3, 3) # can you state the goal state in the grid (row, column) self.hole_states = [(1, 1), (2, 3), (3, 0)] for i, j in self.hole_states: self.grid[i][j] = 1 def reset(self): self.current_state = self.start_state return self.current_state def step(self, action): # could you please set the actions as numbers i, j = self.current_state if action == 0: # move up i = max(i-1, 0) elif action == 1: # move down i = min(i+1, self.size-1) elif action == 2: # move left j = max(j-1, 0) elif action == 3: # move right j = min(j+1, self.size-1) self.current_state = (i, j) # what is your rewads? if self.current_state == self.goal_state: reward = 1 # winning reward? done = True elif self.current_state in self.hole_states: reward = -1 # losing for hole states, reward? done = True else: reward = 0 # else? done = False return self.current_state, reward, done def render(self): print(&#39; n&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) elif (i, j) == self.goal_state: print(&#39;G&#39;, end=&#39; &#39;) else: print(&#39;.&#39;, end=&#39; &#39;) elif self.grid[i][j] == 1: if (i, j) == self.current_state: print(&#39;S&#39;, end=&#39; &#39;) else: print(&#39;X&#39;, end=&#39; &#39;) print() print() def show_q_table(self, q_table): print(&#39;--&#39;) print(&#39;Q-Table:&#39;) print(&#39;--&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: print( &#39;%.2f&#39; % q_table[i][j][0], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][1], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][2], end=&#39; t&#39;) print(&#39;%.2f&#39; % q_table[i][j][3]) else: print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;, end=&#39; t&#39;) print(&#39;NULL&#39;) print() # In one text line show the policy (the sequence of actions that agent take ) def show_policy(self, q_table): print(&#39; n Policy:&#39;) for i in range(self.size): for j in range(self.size): if self.grid[i][j] == 0: action = np.argmax(q_table[i][j]) if action == 0: print(&#39;UP&#39;, end=&#39; &#39;) elif action == 1: print(&#39;DOWN&#39;, end=&#39; &#39;) elif action == 2: print(&#39;LEFT&#39;, end=&#39; &#39;) elif action == 3: print(&#39;RIGTH&#39;, end=&#39; &#39;) else: print(&#39;STAY&#39;, end=&#39; &#39;) . Next, we create an instance of the environment and initialize the Q-table with zeros. . env = FrozenLake() q_table = np.zeros((env.size, env.size, 4)) . Hyperparameters . We then set some hyperparameters for the Q-learning algorithm, such as the number of episodes to run, the maximum number of steps per episode, the learning rate, the discount factor, the starting exploration rate (epsilon), the minimum exploration rate, and the rate at which epsilon decays over time. . # Could you please set your hyperparameters? num_episodes = 2000 max_steps_per_episode = 10 learning_rate = 0.05 discount_factor = 0.99 epsilon = 1.0 min_epsilon = 0.01 epsilon_decay_rate = 0.001 . We define an epsilon-greedy policy for selecting actions, which chooses a random action with probability epsilon or the greedy action (i.e., the action with the highest Q-value) with probability 1 - epsilon. . . # Define epsilon-greedy policy def epsilon_greedy_policy(state): if random.uniform(0, 1) &lt; epsilon: return random.randint(0, 3) else: return np.argmax(q_table[state[0]][state[1]]) . We train the agent by running a loop over the specified number of episodes. In each episode, we start by resetting the environment and selecting actions according to the epsilon-greedy policy. We then update the Q-values for the current state-action pair using the Q-learning update rule. Finally, we update the current state and repeat until the episode ends (either because the agent reaches the goal or exceeds the maximum number of steps). . We decay the exploration rate (epsilon) after each episode to gradually shift the agent from exploration to exploitation over time. . Periodically, we render the current state of the environment and display the current Q-table and policy for visualization. . for episode in range(num_episodes): state = env.reset() done = False t = 0 while not done and t &lt; max_steps_per_episode: action = epsilon_greedy_policy(state) next_state, reward, done = env.step(action) # what is missing to update the Q value? #q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action]) q_table[state[0]][state[1]][action] += learning_rate * (reward + discount_factor * np.max(q_table[next_state[0]][next_state[1]]) - q_table[state[0]][state[1]][action]) state = next_state t += 1 epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay_rate)) # Show progress if episode % 1000 == 0: env.render() env.show_q_table(q_table) env.show_policy(q_table) . . . . . . S . . . . . X X . . G -- Q-Table: -- 0.00 0.00 0.00 0.00 0.00 -0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NULL NULL NULL NULL NULL NULL NULL NULL 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Policy: UP UP UP UP UP STAY UP UP UP UP UP STAY STAY UP UP UP . . . . . S . . . . . X X . . G -- Q-Table: -- 0.00 0.00 0.00 0.00 0.00 -1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -1.00 NULL NULL NULL NULL 0.00 0.00 -0.64 0.00 0.00 -0.19 0.00 0.00 0.00 -0.74 0.00 0.00 -0.26 0.00 0.00 0.00 0.00 0.00 0.00 -0.05 NULL NULL NULL NULL NULL NULL NULL NULL 0.00 0.00 -0.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Policy: UP UP UP UP UP STAY UP UP UP DOWN UP STAY STAY UP UP UP Once training is complete, we test the agent by running a loop until the agent reaches the goal or exceeds the maximum number of steps. In each step, we select the greedy action (i.e., the action with the highest Q-value) and update the current state. We render the environment at each step for visualization. . # Test agent state = env.reset() done = False while not done: action = np.argmax(q_table[state[0]][state[1]]) next_state, reward, done = env.step(action) env.render() state = next_state . S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G S . . . . X . . . . . X X . . G . .",
            "url": "https://sciml-leeds.github.io/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Challenges.html",
            "relUrl": "/workshop/reinforcement_learning/2023/05/05/Reinforcement_learning_Challenges.html",
            "date": " ‚Ä¢ May 5, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://sciml-leeds.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://sciml-leeds.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://sciml-leeds.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Events",
          "content": "Below is an overview of events organised by the SciML community. We aim to put recordings of each event online within a week of them taking place. . Please get in touch if you‚Äôd like to come to talk to us, we‚Äôre a very diverse group in terms of applications in scientific machine learning and would to hear about your work! . Seminars . Title Speaker Date Media . ClimaX: A foundation model for weather and climate | Tung Nguyen (UCLA) | 17/03/2023 15:00 | recording | . Sea ice detection from concurrent visible and SAR imagery using a convolutional neural network | Martin Rogers (British Antarctic Survey) | 24/02/2023 11:00 | recording | . Physics-informed Machine Learning for Trustworthy Climate Emulators | Bj√∂rn L√ºtjens (MIT) | 10/02/2023 14:00 | recording | . Explainable AI for identifying regional climate change patterns | Zack Labe (Princeton) | 13/01/2023 14:00 | recording | . Extending the capabilities of data-driven reduced-order models to make predictions for unseen scenarios | Claire Heaney (Imperial College) | 18/11/2022 11:00 | recording | . Workshops . Title Speaker Date Media . Physics-informed neural networks (PINNs) | Fergus Shone (University of Leeds) | 17/02/2023 14:00 | recording | .",
          "url": "https://sciml-leeds.github.io/events/",
          "relUrl": "/events/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://sciml-leeds.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}